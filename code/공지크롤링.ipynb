{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"공지크롤링.ipynb","provenance":[],"authorship_tag":"ABX9TyO5J9ntEyL1tb+U1MLVV5ny"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Un9U6CXgnvgJ","executionInfo":{"status":"ok","timestamp":1625396857484,"user_tz":-540,"elapsed":26512596,"user":{"displayName":"성균관대학교안예림","photoUrl":"","userId":"13208223441289741912"}}},"source":["from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","def page_text(url):\n","    site = urlopen(\"https://www.skku.edu/skku/campus/skk_comm/notice01.do\" + url)\n","    soup = BeautifulSoup(site.read(), \"html.parser\",from_encoding=\"utf-8\")\n","    if (soup.find('pre', class_ = \"pre\") != None):\n","        page_text = soup.find('pre', class_ = \"pre\").text\n","    elif (soup.find('div', class_ = \"fr-view\") != None):\n","        page_text = soup.find('div', class_ = \"fr-view\").text\n","    else:\n","        page_text = 'None'\n","    return page_text\n","\n","\n","text = [[] for i in range(13260)]\n","i = 0\n","while (i <= 13250):\n","    if (i % 10 == 0):\n","        site = urlopen(\"https://www.skku.edu/skku/campus/skk_comm/notice01.do?mode=list&&articleLimit=10&article.offset=\" + str(i))\n","        soup = BeautifulSoup(site.read(), \"html.parser\",from_encoding=\"utf-8\")\n","        table = soup.find_all('tr')\n","\n","    for k in range(1, len(table)):\n","        sub_table = table[k].find_all('td')\n","        for j in range(0, len(sub_table) - 1):\n","            if j != 1:\n","                text[i].append(sub_table[j].get_text().strip())\n","            if j == 1:\n","                if (sub_table[j].find('span') == None):\n","                    text[i].append('None')\n","                else:\n","                    text[i].append(sub_table[j].find('span').string)\n","                text[i].append(sub_table[j].find('a').string.strip())\n","                text[i].append(sub_table[j].find('a')[\"href\"])\n","                text[i].append(page_text(text[i][-1]))\n","        i += 1\n","\n","df = pd.DataFrame(text, columns = ['no', 'new', 'title', 'link', 'text', 'date', 'visited'])\n","df.to_csv('sample3.csv', encoding='utf-8-sig')"],"execution_count":1,"outputs":[]}]}